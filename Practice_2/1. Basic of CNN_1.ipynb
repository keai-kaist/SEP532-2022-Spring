{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP532 - 인공지능 이론과 실제 (2022 Spring)\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 2 - Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule for Today\n",
    "\n",
    "1. **Basic of CNN** <br>\n",
    "    1-1. Convolutional neural network <br>\n",
    "    1-2. Layers composing CNN <br>\n",
    "    1-3. Image classification using CNN\n",
    "    \n",
    "2. **Transfer learning with pre-trained CNNs** <br>\n",
    "    2-1. Several well-known CNN models <br>\n",
    "    2-2. Tutorial of the pre-trained models using Keras API <br>\n",
    "    2-3. Feature extraction using pre-trained model <br>\n",
    "    2-4. Fine tuning our classification model  <br>\n",
    "    \n",
    "3. **Advanced Models** <br>\n",
    "    3-1. Variational Auto-Encoder (VAE) <br>\n",
    "    3-2. Image Captioning <br>\n",
    "    3-3. Deep Convolutional Generative Adversarial Network (DCGAN) <br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Basic of CNN\n",
    "### 1-0. 왜 CNN 필요한가?\n",
    "- 이미지가 조금만 변형되어도 다른 이미지로 인식 \n",
    "- (!) 이미지 데이터에 대해서 모델의 학습 파라미터가 너무 많이 필요함 -> 높은 computational cost와 overfitting 위험\n",
    "- (+) CNN can preserve spatial information\n",
    "\n",
    "<img src='images/fnn_vs_cnn.png'>\n",
    "\n",
    "\n",
    "### 1-1. Convolutional neural network (CNN)\n",
    "- A convolutional neural networks (CNN) is a class of deep neural networks, most commonly <font color='red'>applied to analyzing visual tasks</font>.\n",
    "- The networks consist of multiple layers of small neuron collections which process portions of the input image, called <font color='red'>receptive fields</font>.\n",
    "    - The connectivity pattern between these neurons is inspired by the organization of the animal visual cortex.\n",
    "- The outputs of these (neural) collections are then tiled so that their input regions overlap, to obtain a <font color='red'>better representation</font> of the original image; this is repeated for every such layer.\n",
    "\n",
    "<!-- <img src=https://i.pinimg.com/originals/2e/94/ce/2e94ce86bdeccca61bef2abf87e0e39c.png width=\"800\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of CNN architecture\n",
    "\n",
    "<img src=http://playagricola.com/Kaggle/extract.png width=\"800\">\n",
    "\n",
    "- CNN consists of a series of <font color='red'> convolutional, non-linear, pooling (downsampling), and fully connected layers</font>.\n",
    "- The convolutional, non-linear and pooling layers are to <font color='blue'>extract a feature map (or activation map)</font>.\n",
    "- The fully connected layer is to <font color='blue'>classify a target using the extracted feature map</font> (e.g. classification a single class of input image or a probability(?) of classes that best describes the image in image classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Layers composing CNN\n",
    "\n",
    "#### Convolutional layer\n",
    "The first layer in a CNN architecture is usually a **convolutional layer**.\n",
    "\n",
    "<img src=https://miro.medium.com/max/790/0*DIPArAJyqAFQwgym width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional filters**\n",
    "\n",
    "<img src=https://devblogs.nvidia.com/wp-content/uploads/2015/11/convolution.png width=\"700\">\n",
    "\n",
    "- A convolutional filter much like a **convolutional kernel or mask** in image processing that a small matrix useful for blurring, sharpening, embossing, edge detection, and more.\n",
    "- This is accomplished by computing a convolution between an filter and an image.\n",
    "- The main difference from a normal filter is that the **convolutional filters are learned</font>**.\n",
    "    - This means that the convolutional filters are learned as a weights during training the CNN models (by backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution** \n",
    "\n",
    "As the filter is sliding, or **<font color='red'>convolving</font>**, around the input image, it multiplies the values in the filter with the original pixel values of the image (a.k.a. computing element-wise multiplication) and add them to get a convolved value.\n",
    "\n",
    "<img src=https://miro.medium.com/max/1800/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif width=\"600\">\n",
    "\n",
    "Now, we repeat this process for every location on the input volume. And, next step would be moving the filter to the right by 1 unit, then right again by 1, and so on. After sliding the filter over all the locations, we are left with an array of numbers usually called an **<font color='red'>activation map</font>** or **<font color='red'>feature map</font>**.\n",
    "\n",
    "<img src=https://taewanmerepo.github.io/2018/01/cnn/conv2.jpg width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stride and Padding**\n",
    "\n",
    "As main parameters of convolutional layer, we can change to modify the behavior of each convolutional layer by using them.\n",
    "\n",
    "Stride: Controls how the filter convolves around the input volume.\n",
    " - In the above example, the filter convolves around the input volume by shifting **one** unit at a time\n",
    " - In that case, the stride was implicitly set at 1.\n",
    " - Stride is normally set in a way so that the output volume is an integer and not a fraction.\n",
    " \n",
    "<img src=https://miro.medium.com/max/1580/1*L4T6IXRalWoseBncjRr4wQ@2x.gif width=\"500\">\n",
    "<img src=https://miro.medium.com/max/1442/1*4wZt9G7W7CchZO-5rVxl5g@2x.gif width=\"500\">\n",
    "\n",
    " \n",
    " \n",
    "Padding\n",
    " - The size of the feature map is smaller than the input, because the convolution filter needs to be contained in the input. \n",
    " - If we want to maintain the same dimensionality, we can use padding to surround the input with zeros.\n",
    " \n",
    "<img src=https://miro.medium.com/max/2126/1*W2D564Gkad9lj3_6t9I2PA@2x.gif width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output size of convolution layer (1D)**\n",
    "\n",
    "<img src='https://i.stack.imgur.com/vD1u3.png' width=\"300\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinearity - ReLU (Rectified Linear Units) layer\n",
    "\n",
    "Now, in a traditional convolutional neural network architecture, there are other layers that are interspersed between these conv layers. <br>\n",
    "(e.g. Input->Conv->ReLU->Conv->ReLU->Pool->Conv->ReLU->Conv->ReLU->Pool->Fully connected)\n",
    "\n",
    "After each conv layer, the **ReLU layer** is convention to apply a *nonlinear layer* (or **activation layer**) immediately afterward. The purpose of this layer is to introduce nonlinearity to a system that basically has just been computing linear operations during the conv layers (just element wise multiplications and summations)\n",
    "\n",
    "**ReLU function**\n",
    "- The **ReLu** function is defined as $f(x) = \\max(0, x)$.\n",
    "- A smooth approximation to the rectifier is the *analytic function*: $f(x) = \\ln(1 + e^x)$, which is called the **softplus** function.\n",
    "- The derivative of softplus is $f'(x) = e^x / (e^x + 1) = 1 / (1 + e^{-x})$, i.e. the **logistic function**.\n",
    "\n",
    "**The advantages of ReLU layer**\n",
    "- It works far better than other nonlinear functions (e.g. tanh and sigmoid), because the network is able to train a lot **faster** (due to computational efficiency) without making a significant difference to the accuracy.\n",
    "- It also helps to alleviate the **vanishing gradient problem**, which is the issue where the lower layers of the network train very slowly because the gradient decreases exponentially through the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### (**참고) Activation function 비교\n",
    "\n",
    "- 다양한 activation function들이 존재\n",
    "- 딥러닝 모델에서 activation function을 쓰는 이유는 복잡한 모델로 만들어 주는 역할 \n",
    "    - 따라서, 주로 non-linear function을 사용 (e.g., sigmoid, tanh, etc.)\n",
    "    - 만약 모델이 linear하게만 구성되면, 복잡한 데이터에 대해서 학습이 안됨 (이 경우, 결국 y=wx=b의 모델과 같음)\n",
    "    - 즉, 모델을 더 복잡하게 만들어주어야 성능을 높일 수 있음\n",
    "\n",
    "**Sigmoid function**\n",
    "- activation function으로서, 각 layer의 복잡도 올려줌\n",
    "- output function으로서, 0~1사이의 값을 출력 (보통 binary classification에 사용) -> squash function\n",
    "    - output function의 경우, 최근에는 주로 softmax function을 사용 (특히 multi-class classification)\n",
    "- **문제점**\n",
    "    1. Back propagation에서 미분값이 너무 작아져 (거의 0에 근접) weight가 업데이트가 사라짐 (saturation 현상, vanishing gradient) \n",
    "      => 즉, bottom layer -> top layer 결국 모든 layer의 weight 업데이트가 중지됨\n",
    "    1. activation function 값이 양수 -> 편미분 값도 양수 -> weight 업데이트 부호가 통일 (+ -> +, - -> - // + -> - X)\n",
    "      => Zigzag 현상 발생 -> 업데이트 느려짐\n",
    "    1. np.exp 연산들어가 연산이 느리다 -> 따라서 convolution 연산은 더 오래 걸림\n",
    "    \n",
    "**Tanh function**\n",
    "- sigmoid와 유사하지만, 최소 출력값이 -1 이기때문에, Zigzag 현상이 덜 함\n",
    "- 하지만, vanishing gradient 현상은 존재, 또한 exp 연산이 더 많이 시간 더 오래걸림 \n",
    "\n",
    "<img src=https://taewanmerepo.github.io/2017/12/tanh/020.jpg width=\"700\">\n",
    "\n",
    "<img src=https://blog.paperspace.com/content/images/size/w1050/2018/06/relu_problem_bg-1.png width=\"700\">\n",
    "\n",
    "\n",
    "**ReLU function**\n",
    "- 가장 많이 사용하며, 성능도 가장 좋음\n",
    "- non-linear 하면서 가장 심플한 함수\n",
    "- **saturation 되는 부분 2군데 -> 1군데로 줄었음**\n",
    "- **exp연산이 없어 빠름** (loss 수렴속도가 sigmoid/tanh에 비해 6배 가까이 빨리 수렴)\n",
    "- activation function 출력값 0 or 1이기 때문에 Zigzag 현생은 있음 \n",
    "- 이후 여러 응용 activation function 나옴 (Leacky ReLU, Parametric ReLU, Exponential Linear Unit, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling layer\n",
    "\n",
    "After some ReLU layers, it is customary to apply a **<font color='red'>pooling layer</font>** (aka *downsampling layer*). In this category, there are also several layer options, with **maxpooling** being the most popular. (There are other options such average pooling and L2-norm pooling.)\n",
    "\n",
    "<img src=https://taewanmerepo.github.io/2018/02/cnn/maxpulling.png width=\"600\">\n",
    "\n",
    "\n",
    "**The purpose of pooling layer**\n",
    "- Reduce the amount of parameters\n",
    "    - The pooling layer drastically reduces the spatial dimension (the length and the width but not the depth) of the input volume.\n",
    "    - By reducing the diemnsion, there remain more important features of an input image.\n",
    "- Control an overfitting\n",
    "    - Because reduce the amount of paraeters and complexity of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully-connected layer\n",
    "\n",
    "The last layer is an important one, namely the **Fully-connected Layer**. This was the last class we learned. \n",
    "\n",
    "Basically, a FC layer looks at what high level features most strongly correlate to a particular class. It has particular weights so that when you compute the products between the weights and the previous layer, you get the correct probabilities for the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going further: Convolution Arithmetic\n",
    "\n",
    "If you want to go further with Convolution and you want to fully understand how convolution works with all the details we omitted in this notebook, I strongly suggest to read this terrific paper: [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
