{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP532 - 인공지능 이론과 실제 (2022 Spring)\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Basic \n",
    "\n",
    "## Linear model \n",
    "Let's consider the binary classification.\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist//SEP532-2022-Spring/blob/master/Practice_1/imgs/bt_example.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "To solve above problem, we can define a simple linear model as follows.\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist//SEP532-2022-Spring/blob/master/Practice_1/imgs/linear_model.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "With the linear model, we apply it to binary classification.\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist//SEP532-2022-Spring/blob/master/Practice_1/imgs/lm_classification.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "#### Issues of linear models\n",
    "- Most real-world data is not linearly separable\n",
    "- In other words, any linear models cannot separate regions correctly\n",
    "- Therefore, non-linearities is neceesary to model arbitrary complex functions \n",
    "\n",
    "<img src=\"https://github.com/keai-kaist//SEP532-2022-Spring/blob/master/Practice_1/imgs/issues_lm.png?raw=true\" align=\"center\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP)\n",
    "\n",
    "#### Injecting non-linearity \n",
    "To solve above issues, we can inject the non-linearity into the linear model through the non-linear functions such as **Sigmoid, Hyperbolic Tangent(tanh), Rectified Linear (ReLU).**\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/inject_non_liearity.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/act_functions.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "#### Perceptron: simplified view \n",
    "Perceptron: A Perceptron is an algorithm used for supervised learning of binary classifiers. Binary classifiers decide whether an input, usually represented by a series of vectors, belongs to a specific class. In short, a perceptron is a single-layer neural network. (Defined by Deep AI)\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/perceptrons.png?raw=true\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "#### Multi-Layer Perceptron \n",
    "MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. \n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/mlp.png?raw=true\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "The reason why do we stack more layers is that 1) hidden layers are nonlinear embeddings of the input; 2) The model can embed the data into\n",
    "the linearly separable space.\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/mlp2.png?raw=true\" align=\"center\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network (DNN)\n",
    "By stacking the more and more layers, neural netowrks have representing and modeling ability for given complex data.\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/dnn.png?raw=true\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "#### Example: recognizing handwritten digits\n",
    "\n",
    "Examples of data are as follows (MNIST)\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/handwritten_digits.jpeg?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "To recognize the handwritten digits using DNN, we first preprocess the images in order to feed them to the model and then train the model.\n",
    "\n",
    "**Data representation**\n",
    "Representing a gray-scale image into an array (i.e. a vector)\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/data_representation.png?raw=true\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "**Forward propagation (Embedding viewpoint)**\n",
    "The vectorized image is propagated throught the layers and classified into one of the digits, 0~9. In the case of trained DNN, each layer represents non-linear embedding of input to easily separable space. \n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/fp_mnist.png?raw=true\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "**Recognition viewpoint**\n",
    "The DNN model understands given handwritten digit by combining the abstracted features represented through multiple layers. In other words, DNN learns a hierarchy of features capturing different levels of abstractions. \n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/recognition_viewpoint.png?raw=true\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "It works similarly for other data modalities. In the case of the face image, each layer in the DNN captures hierarchical features like edges -> facial landmarks -> global face.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/rv_examples.png?raw=true\" align=\"center\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training neural network \n",
    "\n",
    "- **Objective:** find a set of parameters that minimize the error on the dataset.\n",
    "- Notations\n",
    "    - Datasets: ${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\dots, (x^{(N)},y^{(N)})}$ for $N$ number of training data\n",
    "    - Parameters: ${\\text{w}^{(1)},\\text{w}^{(2)}, \\dots, \\text{w}^{(L)}}$ for $L$ number of layers\n",
    "    \n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/dnn_training1.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "- **Loss function**\n",
    "    - Measurement on the mismatch between the model prediction and the true label.\n",
    "    - There are many ways to define the degree of mismatch (i.e. misprediction, error).\n",
    "    - Key to “quantify the performance” of the model on the specific task and data.\n",
    "    \n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/loss_func.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "    \n",
    "    \n",
    "- Example of loss function - Mean Squared Error(MSE)\n",
    "    - Regression tasks\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/mse.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "  \n",
    "  \n",
    "- Example of loss function - Binary cross entropy \n",
    "    - Binary classification \n",
    "    - For predicted values, can be interpreted as a probability vector using softmax function \n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/bce.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "#### Optimizing the loss function (Back-propagation)\n",
    "- **Challenges** for optimizing the loss function\n",
    "    - It is highly non-convex and non-concave.\n",
    "    - It is impossible to find the analytical solution.\n",
    "    - 참고: https://ratsgo.github.io/deep%20learning/2017/09/25/gradient/\n",
    "- **Optimiation via gradient descent**\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/gradient_descent.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "- Algorithm (gradient descent)\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/gd_a1.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "- Algorithm (stochastic gradient descent)\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/gd_a2.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "- **Algorithm (minibatch stochastic gradient descent)**\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/gd_a3.png?raw=true\" align=\"center\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "#### Computing gradients of weights in neural network \n",
    "- Chain rule: propagating the gradient across the layers\n",
    "    - Simplest example: two-layer neural network with one hidden node\n",
    "    - $\\hat{y}=f(x;\\text{W})$\n",
    "    \n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/chain_rule.png?raw=true\" align=\"center\" width=\"700\"/>\n",
    "    \n",
    "\n",
    "- Below shows simple fully-connected network \n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/fcn.png?raw=true\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "- Compute the gradient of weights and bias using chain rule\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/computing_graidents.png?raw=true\" align=\"center\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actviation functions\n",
    "Neural network training is preformed by gradient update. During the back-propagation, if the gradient of weights or derivative of nonlinear function will be zero, there is no downstream gradient. Therefore, we need to prevent the zero-gradient, especially, when computing the derivative of a nonlinear function, the gradient goes zero according to types of activation functions.\n",
    "\n",
    "- Sigmoid function \n",
    "    - Pros\n",
    "        - Bounding the activation value range [0,1]\n",
    "    - Cons\n",
    "        - Zero gradient on saturated neurons (vanishing gradient)\n",
    "        - Outputs are not zero-centered (always positive, 0~0.25) -> Zigzag problem, slow update\n",
    "        - Exponetial operation is involved\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/sigmoid.png?raw=true\" align=\"center\" width=\"400\"/>\n",
    "\n",
    "- Hyperbolic Tangent(Tanh) function \n",
    "    - Pros\n",
    "        - Bounding the activation value range [-1,1]\n",
    "        - Outputs are zero centered\n",
    "    - Cons\n",
    "        - Zero gradient on saturated neurons (vanishing gradient)\n",
    "        - Exponetial operation is involved\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/tanh.png?raw=true\" align=\"center\" width=\"400\"/>\n",
    "\n",
    "- Rectified Linear Unit(ReLU) function \n",
    "    - Pros\n",
    "        - No saturation\n",
    "        - Easy to compute\n",
    "    - Cons\n",
    "        - Not zero-centered output\n",
    "        - Zero gradient for negative activations\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/relu.png?raw=true\" align=\"center\" width=\"400\"/>\n",
    "\n",
    "- Other activation functions\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532-2022-Spring/blob/master/Practice_1/imgs/other_act_funcs.png?raw=true\" align=\"center\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
