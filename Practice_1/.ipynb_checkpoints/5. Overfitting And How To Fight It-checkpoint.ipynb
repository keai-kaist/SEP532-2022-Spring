{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP532 - 인공지능 이론과 실제 (2022 Spring)\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Overfitting And How To Fight It\n",
    "\n",
    "**Oveffiting** is that the accuracy of model on the validation data would peak after training for a number of epochs, and would then stagnate or start decreasing.\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532/blob/master/Week1/imgs/overfit_ex.png?raw=true\" align=\"center\" width=\"700\" />\n",
    "\n",
    "\n",
    "Although it's often possible to achieve high accuracy on the training set, what we really want is to develop models that generalize well to a testing set (or data they haven't seen before).\n",
    "\n",
    "Hence, learning how to deal with overfitting is important. To prevent overfitting, the best solution is to use more training data. If it is difficult to increase amount of the data, there are several techniques such as **weight decay (regularization), dropout, and early stopping.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# !pip install -q git+https://github.com/tensorflow/docs\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Higgs Dataset\n",
    "\n",
    "To check the overfitting exampel, we use the Higgs Dataset, which contains 11,000,000 examples, each with 28 features, and a binary class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'http://mlphysics.ics.uci.edu/data/higgs/HIGGS.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB Dataset \n",
    "\n",
    "```python\n",
    "NUM_WORDS = 1000\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "def multi_hot_sequences(sequences, dimension):\n",
    "    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # results[i]의 특정 인덱스만 1로 설정합니다\n",
    "    return results\n",
    "\n",
    "\n",
    "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
    "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the csv records directly from a gzip file with no intermediate decompression step, we use the `tf.data.experimental.CsvDataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That csv reader class returns a list of scalars for each record. The following function repacks that list of scalars into a (feature_vector, label) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_row(*row):\n",
    "    label = row[0]\n",
    "    features = tf.stack(row[1:],1)\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficient training, we make a new `Dataset` that takes batches of 10000-examples, instead of repacking each row indivisually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.8692932  -0.6350818   0.22569026  0.32747006 -0.6899932   0.75420225\n",
      " -0.24857314 -1.0920639   0.          1.3749921  -0.6536742   0.9303491\n",
      "  1.1074361   1.1389043  -1.5781983  -1.0469854   0.          0.65792954\n",
      " -0.01045457 -0.04576717  3.1019614   1.35376     0.9795631   0.97807616\n",
      "  0.92000484  0.72165745  0.98875093  0.87667835], shape=(28,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for features,label in packed_ds.batch(1000).take(1):\n",
    "    print(features[0])\n",
    "    plt.hist(features.numpy().flatten(), bins = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use just the first 1000 samples for validation and the next 10000 for training. The `Dataset.skip` and `Dataset.take` methods make this easy. At the same time, use the `Dataset.cache` method to ensure that the loader doesn't need to re-read the data from the file on each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VALIDATION = int(1e3)\n",
    "N_TRAIN = int(1e4)\n",
    "BUFFER_SIZE = int(1e4)\n",
    "BATCH_SIZE = 500\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
    "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets return individual examples. Use the `.batch` method to create batches of an appropriate size for training. Before batching also remember to `.shuffle` and `.repeat` the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate overfitting\n",
    "\n",
    "The simplest way to prevent overfitting is to start with a small model: A model with a small number of learnable parameters (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model's \"capacity\".\n",
    "\n",
    "Intuitively, a model with more parameters will have more \"memorization capacity\" and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data.\n",
    "\n",
    "Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.\n",
    "\n",
    "On the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. There is a balance between \"too much capacity\" and \"not enough capacity\".\n",
    "\n",
    "Unfortunately, there is no magical formula to determine the right size or architecture of your model (in terms of the number of layers, or the right size for each layer). You will have to experiment using a series of different architectures.\n",
    "\n",
    "To find an appropriate model size, it's best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n",
    "Many models train better if you gradually reduce the learning rate during training. Use optimizers.schedules to reduce the learning rate over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=STEPS_PER_EPOCH*1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "def get_optimizer():\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above sets a `schedules.InverseTimeDecay` to hyperbolically decrease the learning rate to 1/2 of the base rate at 1000 epochs, 1/3 at 2000 epochs and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = np.linspace(0,100000)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early stopping:** `callbacks.EarlyStopping` is to avoid long and unnecessary training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, name, optimizer=None, max_epochs=2000):\n",
    "  if optimizer is None:\n",
    "    optimizer = get_optimizer()\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=[\n",
    "                  tf.keras.losses.BinaryCrossentropy(\n",
    "                      from_logits=True, name='binary_crossentropy'),\n",
    "                  'accuracy'])\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    epochs=max_epochs,\n",
    "    validation_data=validate_ds,\n",
    "    callbacks=get_callbacks(name),\n",
    "    verbose=0)\n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "size_histories = {}\n",
    "size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
    "plotter.plot(size_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small model\n",
    "\n",
    "2 hidden layers with 16 units each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = tf.keras.Sequential([\n",
    "    # `input_shape` is only required here so that `.summary` works.\n",
    "    tf.keras.layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dense(16, activation='elu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories['Small'] = compile_and_fit(small_model, 'sizes/Small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium model\n",
    "\n",
    "3 hidden layers with 64 units each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='elu', input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dense(64, activation='elu'),\n",
    "    tf.keras.layers.Dense(64, activation='elu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories['Medium']  = compile_and_fit(medium_model, \"sizes/Medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large model \n",
    "\n",
    "Let's add to this benchmark a network that has much more capacity, far more than the problem would warrant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories['large'] = compile_and_fit(large_model, \"sizes/large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training and validation losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(size_histories)\n",
    "a = plt.xscale('log')\n",
    "plt.xlim([5, max(plt.xlim())])\n",
    "plt.ylim([0.5, 0.7])\n",
    "plt.xlabel(\"Epochs [Log Scale]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid lines show the training loss, and the dashed lines show the validation loss (remember: a lower validation loss indicates a better model).\n",
    "\n",
    "While building a larger model gives it more power, if this power is not constrained somehow it can easily overfit to the training set.\n",
    "\n",
    "In this example, typically, only the \"Tiny\" model manages to avoid overfitting altogether, and each of the larger models overfit the data more quickly. This becomes so severe for the \"large\" model that you need to switch the plot to a log-scale to really see what's happening.\n",
    "\n",
    "This is apparent if you plot and compare the validation metrics to the training metrics.\n",
    "\n",
    "- It's normal for there to be a small difference.\n",
    "- If both metrics are moving in the same direction, everything is fine.\n",
    "- If the validation metric begins to stagnate while the training metric continues to improve, you are probably close to overfitting.\n",
    "- If the validation metric is going in the wrong direction, the model is clearly overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies to prevent Overfitting\n",
    "\n",
    "We use a `\"Tiny\" model` above as a baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer_histories = {}\n",
    "regularizer_histories['Tiny'] = size_histories['Tiny']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weight regularization \n",
    "\n",
    "Common way to mitigate overfitting is to **put constraints on the complexity of a network by forcing its weights only to take small values**, which **makes the distribution of weight values more \"regular\".** This is called **\"_weight regularization_\"**, and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n",
    "\n",
    " - [L1 regularization](https://developers.google.com/machine-learning/glossary/#L1_regularization), where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n",
    "\n",
    " - [L2 regularization](https://developers.google.com/machine-learning/glossary/#L2_regularization), where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the squared \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
    " \n",
    " \n",
    "<img src=\"https://github.com/keai-kaist/SEP532/blob/master/Week1/imgs/reg_eq.png?raw=true\" align=\"center\" width=\"700\" />\n",
    "\n",
    "In `tf.keras`, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's add L2 weight regularization now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='elu',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dense(512, activation='elu',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(512, activation='elu',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(512, activation='elu',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "regularizer_histories['l2'] = compile_and_fit(l2_model, \"regularizers/l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(regularizer_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `\"L2\"` regularized model is now much more competitive with the the \"Tiny\" model. This `\"L2\"` model is also much more resistant to overfitting than the `\"Large\"` model it was based on despite having the same number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dropout\n",
    "\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his students at the University of Toronto.\n",
    "\n",
    "The intuitive explanation for dropout is that because individual nodes in the network cannot rely on the output of the others, each node must output features that are useful on their own.\n",
    "\n",
    "Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training. Let's say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1].\n",
    "\n",
    "The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.\n",
    "\n",
    "<img src=\"https://github.com/keai-kaist/SEP532/blob/master/Week1/imgs/dropout.png?raw=true\" align=\"center\" width=\"600\" />\n",
    "\n",
    "In `tf.keras` you can introduce dropout in a network via the Dropout layer, which gets applied to the output of layer right before.\n",
    "\n",
    "Let's add two Dropout layers in our network to see how well they do at reducing overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "regularizer_histories['dropout'] = compile_and_fit(dropout_model, \"regularizers/dropout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(regularizer_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine L2 + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "                 activation='elu', input_shape=(FEATURES,)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "                 activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "                 activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "                 activation='elu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "regularizer_histories['combined'] = compile_and_fit(combined_model, \"regularizers/combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(regularizer_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other strategies?\n",
    "\n",
    "- Data-augmentation\n",
    "- Batch normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
