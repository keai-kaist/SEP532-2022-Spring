{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP532 - 인공지능 이론과 실제 (2022 Spring)\n",
    "## Deep Learning Practice \n",
    "#### Prof. Ho-Jin Choi\n",
    "#### School of Computing, KAIST\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Intorduction to TensorFlow\n",
    "\n",
    "## Introduction to Tensorflow   \n",
    "- [**TensorFlow**](https://www.tensorflow.org) is a software library, developed by Google Brain Team within Google's Machine Learning Intelligence research organization, for the purposes of conducting machine learning and deep neural network research.\n",
    "- TensorFlow combines the computational algebra of compilation optimization techniques, making easy the calculation of many mathematical expressions that would be difficult to calculate, instead.\n",
    "\n",
    "### Main features\n",
    "* Defining, optimizing, and efficiently calculating mathematical expressions involving multi-dimensional arrays (tensors).\n",
    "* Programming support of **deep neural networks** and machine learning techniques.\n",
    "* Transparent use of GPU computing, automating management and optimization of the same memory and the data used. You can write the same code and run it either on CPUs or GPUs. More specifically, TensorFlow will figure out which parts of the computation should be moved to the GPU.\n",
    "* High scalability of computation across machines and huge data sets.\n",
    "\n",
    "\n",
    "## Handling Tensor based on eager execution\n",
    "\n",
    "TensorFlow is called TensorFlow because it handles the flow (node/mathematical operation) of Tensors (data), which you can think of as multidimensional arrays. In TensorFlow, computations can be thought of as graphs. In the previous version (<1.14) of Tensorflow, we implemented a computation graph thought a session we defined and ran the graphs in the session. But, the latest version does not use the session, it can be used simply through an eager executation.\n",
    "\n",
    "[**Eager execution**](https://www.tensorflow.org/guide/eager) is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow and debug models, and it reduces boilerplate as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow:  2.7.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Install TensorFlow\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "print('Tensorflow: ', tf.__version__)\n",
    "\n",
    "import cProfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Tensorflow 2.0, eager execution is enabled by default.\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "import os\n",
    "\n",
    "gdrive_root = '/gdrive/My Drive'\n",
    "print('In gdrive :', os.listdir(gdrive_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "A **tensor** is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]], shape=(3, 4), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]], shape=(3, 4), dtype=int32) \n",
      "\n",
      "tf.Tensor([1.   2.5  4.6  5.75 9.7 ], shape=(5,), dtype=float32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]], shape=(4, 4), dtype=int64) \n",
      "\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int64) \n",
      "\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Construct a 3x4 tensor, filled with zeros\n",
    "x = tf.zeros([3, 4], tf.int32)\n",
    "print(x, '\\n')\n",
    "\n",
    "# Construct a 3x4 tensor, filled with ones\n",
    "x = tf.ones([3, 4], tf.int32)\n",
    "print(x, '\\n')\n",
    "\n",
    "# Construct a 1-dimension tensor with constant values\n",
    "x = tf.constant([1, 2.5, 4.6, 5.75, 9.7])\n",
    "print(x, '\\n')\n",
    "\n",
    "# Construct a 2x2 tensor with constant values\n",
    "x = tf.constant((np.arange(16).reshape(4, 4)))\n",
    "print(x, '\\n')\n",
    "\n",
    "# Convert a numpy array to tensor\n",
    "x = tf.convert_to_tensor(np.array([1,2,3]))\n",
    "print(x, '\\n')\n",
    "\n",
    "# Obtain numpy value from a tensor:\n",
    "x = x.numpy()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant([\n",
    "    [4, 3],\n",
    "    [1, 1],\n",
    "], dtype=tf.float32, name='A')\n",
    "\n",
    "B = tf.constant([\n",
    "    [3, 5],\n",
    "    [1, 2],\n",
    "], dtype=tf.float32, name='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A + B = tf.Tensor(\n",
      "[[7. 8.]\n",
      " [2. 3.]], shape=(2, 2), dtype=float32)\n",
      "A - B = tf.Tensor(\n",
      "[[ 1. -2.]\n",
      " [ 0. -1.]], shape=(2, 2), dtype=float32)\n",
      "A ⨉ B = tf.Tensor(\n",
      "[[15. 26.]\n",
      " [ 4.  7.]], shape=(2, 2), dtype=float32)\n",
      "A * B = tf.Tensor(\n",
      "[[12. 15.]\n",
      " [ 1.  2.]], shape=(2, 2), dtype=float32)\n",
      "A² = tf.Tensor(\n",
      "[[16.  9.]\n",
      " [ 1.  1.]], shape=(2, 2), dtype=float32)\n",
      "sum(A) = tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "det(A) = tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "inv(A) = tf.Tensor(\n",
      "[[ 1. -3.]\n",
      " [-1.  4.]], shape=(2, 2), dtype=float32)\n",
      "A ⨉ inv(A) = tf.Tensor(\n",
      "[[1. 0.]\n",
      " [0. 1.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('A + B =', tf.add(A, B))\n",
    "print('A - B =', tf.subtract(A, B))\n",
    "print('A ⨉ B =', tf.matmul(A, B))\n",
    "print('A * B =', tf.multiply(A, B))\n",
    "print('A² =', tf.square(A))\n",
    "print('sum(A) =', tf.reduce_sum(A))\n",
    "print('det(A) =', tf.linalg.det(A))\n",
    "print('inv(A) =', tf.linalg.inv(A))\n",
    "print('A ⨉ inv(A) =', tf.matmul(A, tf.linalg.inv(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "Broadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are \"stretched\" automatically to fit larger tensors when running combined operations on them.\n",
    "\n",
    "The simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A + B =  tf.Tensor(\n",
      "[[7. 8.]\n",
      " [2. 3.]], shape=(2, 2), dtype=float32)\n",
      "A - B =  tf.Tensor(\n",
      "[[ 1. -2.]\n",
      " [ 0. -1.]], shape=(2, 2), dtype=float32)\n",
      "A @ B =  tf.Tensor(\n",
      "[[15. 26.]\n",
      " [ 4.  7.]], shape=(2, 2), dtype=float32)\n",
      "A * B =  tf.Tensor(\n",
      "[[12. 15.]\n",
      " [ 1.  2.]], shape=(2, 2), dtype=float32)\n",
      "A / B =  tf.Tensor(\n",
      "[[1.3333334 0.6      ]\n",
      " [1.        0.5      ]], shape=(2, 2), dtype=float32)\n",
      "A + 1 =  tf.Tensor(\n",
      "[[5. 4.]\n",
      " [2. 2.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Operator overloading is supported\n",
    "print('A + B = ', A + B)\n",
    "print('A - B = ', A - B)\n",
    "print('A @ B = ', A @ B) # matrix multiplication\n",
    "print('A * B = ', A * B) # Element-wise \n",
    "print('A / B = ', A / B)\n",
    "\n",
    "# Broadcasting support\n",
    "print('A + 1 = ', tf.add(A, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor shpaes \n",
    "\n",
    "Tensors have shapes. Some vocabulary:\n",
    "\n",
    "- **Shape**: The length (number of elements) of each of the axes of a tensor.\n",
    "- **Rank**: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\n",
    "- **Axis** or **Dimension**: A particular dimension of a tensor.\n",
    "- **Size**: The total number of items in the tensor, the product shape vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_4_tensor = tf.zeros([3, 2, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of every element: <dtype: 'float32'>\n",
      "Number of axes: 4\n",
      "Shape of tensor: (3, 2, 4, 5)\n",
      "Elements along axis 0 of tensor: 3\n",
      "Elements along the last axis of tensor: 5\n",
      "Total number of elements (3*2*4*5):  120\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of every element:\", rank_4_tensor.dtype)\n",
    "print(\"Number of axes:\", rank_4_tensor.ndim)\n",
    "print(\"Shape of tensor:\", rank_4_tensor.shape)\n",
    "print(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0])\n",
    "print(\"Elements along the last axis of tensor:\", rank_4_tensor.shape[-1])\n",
    "print(\"Total number of elements (3*2*4*5): \", tf.size(rank_4_tensor).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(참고) While axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/guide/images/tensor/shape2.png\" align=\"center\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "#### Single-axis indexing\n",
    "TensorFlow follows standard Python indexing rules, similar to indexing a list or a string in Python, and the basic rules for NumPy indexing.\n",
    "\n",
    "- indexes start at `0`\n",
    "- negative indices count backwards from the end\n",
    "- colons, `:`, are used for slices: `start:stop:step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  2  3  5  8 13 21 34]\n"
     ]
    }
   ],
   "source": [
    "rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])\n",
    "print(rank_1_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing with a scalar removes the axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First: 0\n",
      "Second: 1\n",
      "Last: 34\n"
     ]
    }
   ],
   "source": [
    "print(\"First:\", rank_1_tensor[0].numpy())\n",
    "print(\"Second:\", rank_1_tensor[1].numpy())\n",
    "print(\"Last:\", rank_1_tensor[-1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing with a `:` slice keeps the axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything: [ 0  1  1  2  3  5  8 13 21 34]\n",
      "Before 4: [0 1 1 2]\n",
      "From 4 to the end: [ 3  5  8 13 21 34]\n",
      "From 2, before 7: [1 2 3 5 8]\n",
      "Every other item: [ 0  1  3  8 21]\n",
      "Reversed: [34 21 13  8  5  3  2  1  1  0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Everything:\", rank_1_tensor[:].numpy())\n",
    "print(\"Before 4:\", rank_1_tensor[:4].numpy())\n",
    "print(\"From 4 to the end:\", rank_1_tensor[4:].numpy())\n",
    "print(\"From 2, before 7:\", rank_1_tensor[2:7].numpy())\n",
    "print(\"Every other item:\", rank_1_tensor[::2].numpy())\n",
    "print(\"Reversed:\", rank_1_tensor[::-1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-axis indexing\n",
    "\n",
    "Higher rank tensors are indexed by passing multiple indices.\n",
    "\n",
    "The exact same rules as in the single-axis case apply to each axis independently.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "rank_2_tensor = tf.constant([[1, 2],\n",
    "                             [3, 4],\n",
    "                             [5, 6]], dtype=tf.float16)\n",
    "\n",
    "print(rank_2_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing an integer for each index, the result is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Pull out a single value from a 2-rank tensor\n",
    "print(rank_2_tensor[1, 1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second row: [3. 4.]\n",
      "Second column: [2. 4. 6.]\n",
      "Last row: [5. 6.]\n",
      "First item in last column: 2.0\n",
      "Skip the first row:\n",
      "[[3. 4.]\n",
      " [5. 6.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get row and column tensors\n",
    "print(\"Second row:\", rank_2_tensor[1, :].numpy())\n",
    "print(\"Second column:\", rank_2_tensor[:, 1].numpy())\n",
    "print(\"Last row:\", rank_2_tensor[-1, :].numpy())\n",
    "print(\"First item in last column:\", rank_2_tensor[0, -1].numpy())\n",
    "print(\"Skip the first row:\")\n",
    "print(rank_2_tensor[1:, :].numpy(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "rank_3_tensor = tf.constant([\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9]],\n",
    "  [[10, 11, 12, 13, 14],\n",
    "   [15, 16, 17, 18, 19]],\n",
    "  [[20, 21, 22, 23, 24],\n",
    "   [25, 26, 27, 28, 29]],])\n",
    "\n",
    "print(rank_3_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 4  9]\n",
      " [14 19]\n",
      " [24 29]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(rank_3_tensor[:, :, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/guide/images/tensor/index1.png\" align=\"center\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Shapes\n",
    "\n",
    "Reshaping a tensor is of great utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Shape returns a `TensorShape` object that shows the size along each axis\n",
    "x = tf.constant([[1], [2], [3]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1]\n"
     ]
    }
   ],
   "source": [
    "# Convert this object into a Python list\n",
    "print(x.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape a tensor into a new shape. The `tf.reshape` operation is fast and cheap as the underlying data does not need to be duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reshape a tensor to a new shape.\n",
    "reshaped = tf.reshape(x, [1, 3])\n",
    "\n",
    "print(x.shape)\n",
    "print(reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style \"row-major\" memory ordering, where incrementing the rightmost index corresponds to a single step in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(rank_3_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we flatten a tensor we can see what order it is laid out in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29], shape=(30,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# A `-1` passed in the `shape` argument says \"Whatever fits\".\n",
    "print(tf.reshape(rank_3_tensor, [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ypically the only reasonable use of `tf.reshape` is to combine or split adjacent axes (or add/remove 1s).\n",
    "\n",
    "For this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]\n",
      " [25 26 27 28 29]], shape=(6, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]\n",
      " [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reshape(rank_3_tensor, [3*2, 5]))\n",
    "print(tf.reshape(rank_3_tensor, [3, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/guide/images/tensor/reshape-before.png\" align=\"center\" /> <img src=\"https://www.tensorflow.org/guide/images/tensor/reshape-good1.png\" align=\"center\" /> <img src=\"https://www.tensorflow.org/guide/images/tensor/reshape-good2.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String tesnors \n",
    "\n",
    "`tf.string` is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\n",
    "\n",
    "The strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See `tf.strings` for functions to manipulate them.\n",
    "\n",
    "Here is a scalar string tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Gray wolf', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Tensors can be strings, too here is a scalar string.\n",
    "scalar_string_tensor = tf.constant(\"Gray wolf\")\n",
    "print(scalar_string_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above printout the `b` prefix indicates that `tf.string` dtype is not a unicode string, but a byte-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# If we have three string tensors of different lengths, this is OK.\n",
    "tensor_of_strings = tf.constant([\"Gray wolf\",\n",
    "                                 \"Quick brown fox\",\n",
    "                                 \"Lazy dog\"])\n",
    "\n",
    "# Note that the shape is (3,). The string length is not included.\n",
    "print(tensor_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/guide/images/tensor/strings.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain the numbers in the string by utilizing `tf.string.to_number`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  1.  10. 100.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "text = tf.constant(\"1 10 100\")\n",
    "print(tf.strings.to_number(tf.strings.split(text, \" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor slicing \n",
    " \n",
    "#### Extract tensor slices\n",
    "Perform NumPy-like tensor slicing using `tf.slice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6 7], shape=(8,), dtype=int32)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.constant([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(t1)\n",
    "\n",
    "print(tf.slice(t1,\n",
    "               begin=[1],\n",
    "               size=[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use a more Pythonic syntax. Note that tensor slices are evenly spaced over a start-stop range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1[1:4], '\\n')\n",
    "print(t1[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `tf.slice` on higher dimensional tensors as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.constant([[[1, 3, 5, 7],\n",
    "                   [9, 11, 13, 15]],\n",
    "                  [[17, 19, 21, 23],\n",
    "                   [25, 27, 29, 31]]\n",
    "                  ])\n",
    "print(t2)\n",
    "print(tf.slice(t2,\n",
    "               begin=[1, 1, 0],\n",
    "               size=[1, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `tf.strided_slice` to extract slices of tensors by 'striding' over the tensor dimensions.\n",
    "\n",
    "Use `tf.gather` to extract specific indices from a single axis of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 3 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.gather(t1,\n",
    "                indices=[0, 3, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/guide/images/tf_slicing/slice_1d_3.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = tf.constant(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "\n",
    "print(tf.gather(alphabet,\n",
    "                indices=[2, 0, 19, 18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/guide/images/tf_slicing/gather_1.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract slices from multiple axes of a tensor, use `tf.gather_nd`. This is useful when you want to gather the elements of a matrix as opposed to just its rows or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = tf.constant([[0, 5],\n",
    "                  [1, 6],\n",
    "                  [2, 7],\n",
    "                  [3, 8],\n",
    "                  [4, 9]])\n",
    "print(t3)\n",
    "print(tf.gather_nd(t3,\n",
    "                   indices=[[2], [3], [0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/guide/images/tf_slicing/gather_2.png\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]]\n"
     ]
    }
   ],
   "source": [
    "t4 = np.reshape(np.arange(18), [2, 3, 3])\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0 16], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(tf.gather_nd(t4,\n",
    "                   indices=[[0, 0, 0], [1, 2, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [15 16 17]]], shape=(2, 2, 3), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(tf.gather_nd(t4,\n",
    "                   indices=[[[0, 0], [0, 2]], [[1, 0], [1, 2]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert data into tensors\n",
    "\n",
    "Use `tf.scatter_nd` to insert data at specific slices/indices of a tensor. Note that the tensor into which you insert values is zero-initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  0  4  0  6  0  8  0 10], shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t6 = tf.constant([10])\n",
    "indices = tf.constant([[1], [3], [5], [7], [9]])\n",
    "data = tf.constant([2, 4, 6, 8, 10])\n",
    "\n",
    "print(tf.scatter_nd(indices=indices,\n",
    "                    updates=data,\n",
    "                    shape=t6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 2 11 18], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t2 = tf.constant([[0, 1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8, 9],\n",
    "                  [10, 11, 12, 13, 14],\n",
    "                  [15, 16, 17, 18, 19]])\n",
    "\n",
    "new_indices = tf.constant([[0, 2], [2, 1], [3, 3]])\n",
    "t7 = tf.gather_nd(t2, indices=new_indices)\n",
    "print(t7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0  0  2  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0 11  0  0  0]\n",
      " [ 0  0  0 18  0]], shape=(4, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t8 = tf.scatter_nd(indices=new_indices, updates=t7, shape=tf.constant([4, 5]))\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert data into a tensor with pre-existing values, use `tf.tensor_scatter_nd_add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 7 6]\n",
      " [9 5 1]\n",
      " [4 3 8]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t11 = tf.constant([[2, 7, 0],\n",
    "                   [9, 0, 1],\n",
    "                   [0, 3, 8]])\n",
    "\n",
    "# Convert the tensor into a magic square by inserting numbers at appropriate indices\n",
    "\n",
    "t12 = tf.tensor_scatter_nd_add(t11,\n",
    "                               indices=[[0, 2], [1, 1], [2, 0]],\n",
    "                               updates=[6, 5, 4])\n",
    "\n",
    "print(t12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, use `tf.tensor_scatter_nd_sub` to subtract values from a tensor with pre-existing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Convert the tensor into an identity matrix\n",
    "t13 = tf.tensor_scatter_nd_sub(t11,\n",
    "                               indices=[[0, 0], [0, 1], [1, 0], [1, 1], [1, 2], [2, 1], [2, 2]],\n",
    "                               updates=[1, 7, 9, -1, 1, 3, 7])\n",
    "\n",
    "print(t13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tf.tensor_scatter_nd_min` to copy element-wise minimum values from one tensor to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-2 -7 -6]\n",
      " [-9 -5  1]\n",
      " [-4 -3 -8]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t14 = tf.constant([[-2, -7, 0],\n",
    "                   [-9, 0, 1],\n",
    "                   [0, -3, -8]])\n",
    "\n",
    "t15 = tf.tensor_scatter_nd_min(t14,\n",
    "                               indices=[[0, 2], [1, 1], [2, 0]],\n",
    "                               updates=[-6, -5, -4])\n",
    "\n",
    "print(t15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, use `tf.tensor_scatter_nd_max` to copy element-wise maximum values from one tensor to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-2 -7  6]\n",
      " [-9  5  1]\n",
      " [ 4 -3 -8]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "t16 = tf.tensor_scatter_nd_max(t14,\n",
    "                               indices=[[0, 2], [1, 1], [2, 0]],\n",
    "                               updates=[6, 5, 4])\n",
    "\n",
    "print(t16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic control flow\n",
    "\n",
    "As you've seen, TensorFlow now supports an imperative programming style, and that's all because of Eager. As another example of the power of Eager, let's take a look at how we can build a dynamic model that uses Python flow control. Here's an example of the [Fizz buzz](https://en.wikipedia.org/wiki/Fizz_buzz) using TensorFlow’s arithmetic operations. Such dynamic behavior is not possible in past versions of TensorFlow (up to v1.4)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizzbuzz(max_num):\n",
    "    counter = tf.constant(0)\n",
    "    max_num = tf.convert_to_tensor(max_num)\n",
    "    \n",
    "    for num in range(1, max_num.numpy() + 1):\n",
    "        num = tf.constant(num)\n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3) == 0:\n",
    "            print('Fizz')\n",
    "        elif int(num % 5) == 0:\n",
    "            print('Buzz')\n",
    "        else:\n",
    "            print(num.numpy())\n",
    "            \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Fizz\n",
      "4\n",
      "Buzz\n",
      "Fizz\n",
      "7\n",
      "8\n",
      "Fizz\n",
      "Buzz\n",
      "11\n",
      "Fizz\n",
      "13\n",
      "14\n",
      "FizzBuzz\n"
     ]
    }
   ],
   "source": [
    "fizzbuzz(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU-based Computiation \n",
    "\n",
    "On a typical system, there are multiple computing devices. \n",
    "\n",
    "In TensorFlow, the supported device types are **CPU** and **GPU**. \n",
    "\n",
    "They are represented as strings. For example:\n",
    "\n",
    "* `'/cpu:0'`: The CPU of your machine.\n",
    "* `'/gpu:0'`: The GPU of your machine, if you have one.\n",
    "* `'/gpu:1'`: The second GPU of your machine, etc.\n",
    "    \n",
    "If a TensorFlow operation has both **CPU** and **GPU** implementations, the GPU devices will be given priority when the operation is assigned to a device. \n",
    "\n",
    "For example, `matmul` has both CPU and GPU kernels. On a system with devices `cpu:0` and `gpu:0`, `gpu:0` will be selected to run `matmul`.    \n",
    "\n",
    "### Using multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0', '/device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "available_gpus = get_available_gpus()\n",
    "print(available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-46-f7688b595fc2>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "tf.Tensor(\n",
      "[[ 44.  56.]\n",
      " [ 98. 128.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "if tf.test.is_gpu_available():\n",
    "    for d in available_gpus:\n",
    "        with tf.device(d):\n",
    "            a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "            b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "            c.append(tf.matmul(a, b))\n",
    "            \n",
    "    with tf.device('/cpu:0'):\n",
    "        result = tf.add_n(c)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.2850s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.random.normal((512, 64, 32, 32))\n",
    "    b = tf.random.normal((512, 64, 32, 32))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tf.matmul(a, b)\n",
    "    end_time = time.time()\n",
    "\n",
    "print('Elapsed time: {:.4f}s'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.0002s\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.random.normal((512, 64, 32, 32))\n",
    "    b = tf.random.normal((512, 64, 32, 32))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tf.matmul(a, b)\n",
    "    end_time = time.time()\n",
    "    \n",
    "print('Elapsed time: {:.4f}s'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### More on Tensorflow\n",
    "\n",
    "[Official API Documentation](https://www.tensorflow.org/tutorials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
